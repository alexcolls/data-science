{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "\n",
    "from sklearn import set_config; set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a datascientist, you don't have access to the full dataset, only the 100k on which you've been tasked to train & finetune the best model)\n",
    "- As ML Engineer, you'll have access to the full dataset later, but not for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"~/.lewagon/mlops/data/raw/train_100k.csv\"\n",
    "df = pd.read_csv(DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) compress data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compress our dataframe by lowering it's numeric dtypes\n",
    "- from  `float64` to `float32`\n",
    "- from `int64` to `int8`\n",
    "\n",
    "To do so, we iterate on its columns, and for each one, reduce it's `dtypes` as much as possible using [`pd.to_numeric`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html)\n",
    "\n",
    "**❓ Read more about dtypes compression in ML-Ops Train-at-scale lecture on Kitt, \"Appendix A1: Memory Optimization\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024**2\n",
    "    print(\"old dataframe size: \", round(input_size,2), 'MB')\n",
    "    \n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for t in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=t))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=t)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "    \n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024**2,2), \" MB\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compress(df, verbose=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check dtypes optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force optimal dtype directly at loading to minimize RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES_RAW_OPTIMIZED = {\n",
    "    \"key\": \"O\",\n",
    "    \"fare_amount\": \"float32\",\n",
    "    \"pickup_datetime\": \"O\",\n",
    "    \"pickup_longitude\": \"float32\",\n",
    "    \"pickup_latitude\": \"float32\",\n",
    "    \"dropoff_longitude\": \"float32\",\n",
    "    \"dropoff_latitude\": \"float32\",\n",
    "    \"passenger_count\": \"int8\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(DATA_URL, dtype=DTYPES_RAW_OPTIMIZED)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant columns or rows\n",
    "df = df.drop(columns=['key'])\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any', axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove buggy transactions\n",
    "df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0) |\n",
    "        (df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "df = df[df.passenger_count > 0]\n",
    "df = df[df.fare_amount > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check NYC bouding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image of NYC map\n",
    "bounding_boxes = (-74.3, -73.7, 40.5, 40.9)\n",
    "url = 'https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/nyc_-74.3_-73.7_40.5_40.9.png'\n",
    "nyc_map = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "plt.imshow(nyc_map);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant/non-representative transactions (rows) for a training set\n",
    "df = df[df[\"pickup_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"dropoff_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"pickup_longitude\"].between(left=-74.3, right=-73.7)]\n",
    "df = df[df[\"dropoff_longitude\"].between(left=-74.3, right=-73.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cap training set to reasonable values \n",
    "df = df[df.fare_amount < 400]\n",
    "df = df[df.passenger_count < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of fare\n",
    "df.fare_amount.hist(bins=100, figsize=(14,3))\n",
    "plt.xlabel('fare $USD')\n",
    "plt.title('Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used more often to plot data on the NYC map\n",
    "def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,10))\n",
    "    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='red', s=s)\n",
    "    axs[0].set_xlim((BB[0], BB[1]))\n",
    "    axs[0].set_ylim((BB[2], BB[3]))\n",
    "    axs[0].set_title('Pickup locations')\n",
    "    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n",
    "\n",
    "    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='blue', s=s)\n",
    "    axs[1].set_xlim((BB[0], BB[1]))\n",
    "    axs[1].set_ylim((BB[2], BB[3]))\n",
    "    axs[1].set_title('Dropoff locations')\n",
    "    axs[1].imshow(nyc_map, zorder=0, extent=BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training data on map\n",
    "plot_on_map(df, bounding_boxes, nyc_map, s=1, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_on_map(df, bounding_boxes, nyc_map, s=20, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hires(df, BB, figsize=(12, 12), ax=None, c=('r', 'b')):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    def select_within_boundingbox(df, BB):\n",
    "        return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "            (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "            (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "            (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "            \n",
    "    idx = select_within_boundingbox(df, BB)\n",
    "    ax.scatter(df[idx].pickup_longitude, df[idx].pickup_latitude, c=\"red\", s=0.01, alpha=0.5)\n",
    "    ax.scatter(df[idx].dropoff_longitude, df[idx].dropoff_latitude, c=\"blue\", s=0.01, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74.1, -73.7, 40.6, 40.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74, -73.95, 40.7, 40.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Baseline Score  - preliminary intuitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the haverzine and manhattan distance between two points on the earth (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    Computes distance in kms\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "    \n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "    \n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "    \n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "    \n",
    "    return manhattan_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance'] = manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")\n",
    "df['distance'].hist(bins=50)\n",
    "plt.title(\"distance (km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='distance', y='fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "r2, pvalue = pearsonr(df['distance'], df['fare_amount'])\n",
    "print(f'{r2=}')\n",
    "print(f'{pvalue=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "mae = -1*cross_val_score(LinearRegression(), X=df[['distance']], y=df['fare_amount'], scoring='neg_mean_absolute_error').mean()\n",
    "print(f'{mae=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ We've got our baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['distance'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a dataset with only 5 features (passengers + lon/lat), and potentially dozens of millions of rows.\n",
    "\n",
    "👉 It make perfect sense to create a lot of \"engineered\" features such as \"hour of the day, etc...\"  \n",
    "- Hundreds of them would cause no problem because the huge number of rows will allow our model to learn all weights associated with these multiple features\n",
    "- A dense, deep learning network will be well suited for such case\n",
    "\n",
    "👇 The proposed preprocessor outputs a **fixed** number of features (65) that is independent of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"fare_amount\", axis=1)\n",
    "y = df[[\"fare_amount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Passenger preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse passengers numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.passenger_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1\n",
    "p_max = 8\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p-p_min)/(p_max-p_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Time Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract category attributes from the \"pickup_datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def transform_time_features(X: pd.DataFrame)->np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    pickup_dt = pd.to_datetime(X[\"pickup_datetime\"],\n",
    "                                format=\"%Y-%m-%d %H:%M:%S UTC\",\n",
    "                                utc=True)\n",
    "    pickup_dt = pickup_dt.dt.tz_convert(\"America/New_York\").dt\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "    year = pickup_dt.year\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "    \n",
    "    return np.stack([hour_sin, hour_cos, dow, month, year], axis=1)\n",
    "\n",
    "X_time_processed = transform_time_features(X[[\"pickup_datetime\"]])\n",
    "\n",
    "pd.DataFrame(X_time_processed, columns=[\"hour_sin\", \"hour_cos\", \"dow\", \"month\", \"year\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, one-hot-encode [\"day of week\", \"month\"] by forcing all 24*7 combinations of categories to be always present in X_processed (we want a fixed size for X_processed at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_categories = {\n",
    "        0: np.arange(0, 7, 1),  # days of the week from 0 to 6\n",
    "        1: np.arange(1, 13, 1)  # months of the year from 1 to 12\n",
    "    }\n",
    "\n",
    "OneHotEncoder(categories=time_categories, sparse=False)\\\n",
    "    .fit_transform(X_time_processed[:,[2,3]]) # [2,3] for ['dow', 'month'] !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine this with rescaling of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.pickup_datetime.min())\n",
    "print(df.pickup_datetime.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_min = 2009\n",
    "year_max = 2019 # Our model may extend in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse=False,\n",
    "            handle_unknown=\"ignore\"), [2,3]), # correspond to columns [\"day of week\", \"month\"], not the others columns\n",
    "        (FunctionTransformer(lambda year: (year-year_min)/(year_max-year_min)), [4]), # min-max scale the columns 4 [\"year\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "        )\n",
    "    )\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train)).plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ 23 features approximately centered and scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Distance pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add both haversine and manhattan distances as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the haverzine and manhattan distance between two points on the earth (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    Computes distance in kms\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "    \n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "    \n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "    \n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "    \n",
    "    a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "    haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_km = haversine_rad * earth_radius\n",
    "    \n",
    "    return dict(\n",
    "        haversize=haversine_km,\n",
    "        manhattan=manhattan_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X:pd.DataFrame)-> pd.DataFrame:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    res = distances_vectorized(X, *lonlat_features)\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "distances = transform_lonlat_features(X[lonlat_features])\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_min = 0\n",
    "dist_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min)/(dist_max - dist_min))\n",
    "    )\n",
    "distance_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "X_processed.plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ 25 features, approximately scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) GeoHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add information about **districts**! \n",
    "\n",
    "Some might be more expensive than others to go/depart from (e.g JFKairport!)\n",
    "\n",
    "In order to _bucketize_ geospacial information, we'll use `pygeohash` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as gh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 pygeohash converts (lat,lon) into geospacial \"squared buckets\" of chosen precisions. The more precision you ask, the more \"buckets\" possibility there is!\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/geohashes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = X_train.iloc[0,:]\n",
    "(x0.pickup_latitude, x0.pickup_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=3))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=4))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Let's apply it to ALL our dataset (note that this preprocessing may take a very long time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohashes = pd.concat([\n",
    "    X_train.apply(lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=5), axis=1),\n",
    "    X_train.apply(lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=5), axis=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(geohashes.value_counts()))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.cumsum(geohashes.value_counts()[:20])/(2*len(X_train))*100)\n",
    "plt.title(\"percentage of taxi rides from/to these districts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ Only the 20 first district matters. We can one hot encode these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_geohash_districts = np.array(geohashes.value_counts()[:20].index)\n",
    "most_important_geohash_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hard-code below the 20 most frequent district geohash of precision 5,\n",
    "# covering about 99% of all dropoff/pickup location.\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X:pd.DataFrame, precision:int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a geohash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon,lat) tuple, for pick-up, and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(lambda x: gh.encode(\n",
    "        x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "                                    axis=1)\n",
    "    X[\"geohash_dropoff\"] = X.apply(lambda x: gh.encode(\n",
    "        x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "                                    axis=1)\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_categories = {\n",
    "    0: most_important_geohash_districts,  # pickup district list\n",
    "    1: most_important_geohash_districts  # dropoff district list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(categories=geohash_categories,\n",
    "                  handle_unknown=\"ignore\",\n",
    "                  sparse=False))\n",
    "geohash_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Full Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap in one cell our final preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1\n",
    "p_max = 8\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p - p_min) /\n",
    "                                        (p_max - p_min))\n",
    "\n",
    "# DISTANCE PIPE\n",
    "dist_min = 0\n",
    "dist_max = 100\n",
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min)/(dist_max - dist_min))\n",
    ")\n",
    "\n",
    "# TIME PIPE\n",
    "year_min = 2009\n",
    "year_max = 2019\n",
    "time_categories = {\n",
    "    0: np.arange(0, 7, 1),  # days of the week\n",
    "    1: np.arange(1, 13, 1)  # months of the year\n",
    "}\n",
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse=False,\n",
    "            handle_unknown=\"ignore\"), [2,3]), # correspond to columns [\"day of week\", \"month\"], not the others columns\n",
    "        (FunctionTransformer(lambda year: (year-year_min)/(year_max-year_min)), [4]), # min-max scale the columns 4 [\"year\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "        )\n",
    "    )\n",
    "\n",
    "# GEOHASH PIPE\n",
    "lonlat_features = [\n",
    "    \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\",\n",
    "    \"dropoff_longitude\"\n",
    "]\n",
    "\n",
    "# Below are the 20 most frequent district geohash of precision 5,\n",
    "# covering about 99% of all dropoff/pickup location,\n",
    "# according to prior analysis in a separate notebook\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]\n",
    "\n",
    "geohash_categories = {\n",
    "    0: most_important_geohash_districts,  # pickup district list\n",
    "    1: most_important_geohash_districts  # dropoff district list\n",
    "}\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(categories=geohash_categories,\n",
    "                    handle_unknown=\"ignore\",\n",
    "                    sparse=False))\n",
    "\n",
    "# COMBINED PREPROCESSOR\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "        (\"geohash\", geohash_pipe, lonlat_features),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = final_preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "pd.DataFrame(X_train_processed).plot(kind='box', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.heatmap(pd.DataFrame(X_train_processed).corr(), vmin=-1, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we compress our data to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_processed.nbytes / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress a bit the data\n",
    "X_train_processed = X_train_processed.astype(np.float32)\n",
    "print(X_train_processed.nbytes / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ The preprocessor outputs a **fixed** number of features (65) that is independent of the training set. \n",
    "\n",
    "☝️ The preprocessor is also  **state-less** (i.e it has no `.fit()` method, only a `.transform()`).  \n",
    " It can be seen as a *pure function* $f:X \\rightarrow X_{processed}$ without internal state (as opposed to standard scaling for instance, which has to store \"X_train standard deviations\" as internal states)\n",
    "\n",
    "These two feature will make work much easier for the ML Engineering team to scale preprocessing to hundreds of Go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ To begin with, please install the tensorflow version that corresponds to your processor\n",
    "\n",
    "In your `requirements.txt`, add \n",
    "- tensorflow==2.8.0 (if you are on Intel processor)\n",
    "- tensorflow-macos==2.8.0 (if you are on M1)\n",
    "\n",
    "Then `pip install -e .` to install it on `taxifare-env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(X: np.ndarray) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    reg = regularizers.l1_l2(l2=0.01)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.BatchNormalization(input_shape=X.shape[1:]))\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg, input_shape=X.shape[1:]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Dense(50, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.99)) # use momentum=0 for to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    \n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    print(\"✅ model initialized\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_train_processed)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\",\n",
    "                    patience=2,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=0)\n",
    "\n",
    "history = model.fit(X_train_processed,\n",
    "                    y_train,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=100,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[es],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed = final_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_pred, label='pred', color='r', bins=200, alpha=0.3)\n",
    "plt.hist(y_test, label='truth', color='b', bins=200, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim((0,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test\n",
    "sns.histplot(residuals)\n",
    "plt.xlim(xmin=-20, xmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.sort_values(by='fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. Actual scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_test,y=residuals, alpha=0.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. Predicted scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_pred,y=residuals, alpha=0.1)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ Our model has MAE of about 2$ per course, compared with a mean course price of 11$.  \n",
    "\n",
    "A simple linear regression would give us about 2.5$ of MAE, but the devil lies in the details!\n",
    "\n",
    "In particular, we're not that good at predicting very long / expensive courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Test your understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Try answer these questions with your buddy\n",
    "1. Are you satisfied with the model performance ?\n",
    "2. What is a state-less pipeline (as opposed to state-full) ?\n",
    "3. How does a OHEncoder works with fixed column categories ?\n",
    "4. How is the data-normalization done in the Neural Net ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary markdown='span'>💡 Answers</summary>\n",
    "\n",
    "1. We have a 25% improvement over the linear regression baseline, and it seems that our model has been doing its best, given the features it has been provided with. Besides, a 2$ forecast error on a taxi courses whose prices also depends on traffic seems close to the irreducible error rate.\n",
    "2. A state-less pipeline has no real `.fit()` method, only a `.transform()`. \n",
    "3. To become state-less, we've hard-coded the `categories` to one-hot-encode `OneHotEncoder(categories=categories,...)` and hard-coded the statistical features of each columns in our scalers:  `FunctionTransformer(lambda dist: (dist - dist_min)/(dist_max - dist_min))`\n",
    "4. In the tensorflow model, notice the `layers.BatchNormalization()` we've added between each dense layers, which normalizes data batch-per-batch! It's a cool feature to help fix vanish gradients during the back-propagation!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Predict the price for this new course `X_new` below and store the result `y_new` as a `np.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(dict(\n",
    "    key=[\"2013-07-06 17:18:00\"],  # useless but the pipeline requires it\n",
    "    pickup_datetime=[\"2013-07-06 17:18:00 UTC\"],\n",
    "    pickup_longitude=[-73.950655],\n",
    "    pickup_latitude=[40.783282],\n",
    "    dropoff_longitude=[-73.984365],\n",
    "    dropoff_latitude=[40.769802],\n",
    "    passenger_count=[1]))\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "import os\n",
    "\n",
    "result = ChallengeResult('notebook',subdir='train_at_scale',\n",
    "    y_new=y_new\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
