from tests.test_base import write_result
from taxifare.ml_logic.data import clean_data

from taxifare.ml_logic.params import (CHUNK_SIZE,
                                            DTYPES_RAW_OPTIMIZED_HEADLESS,
                                            DTYPES_RAW_OPTIMIZED,
                                            DTYPES_PROCESSED_OPTIMIZED,
                                            COLUMN_NAMES_RAW,
                                            DATASET_SIZE,
                                            VALIDATION_DATASET_SIZE,
                                            LOCAL_DATA_PATH)

from taxifare.ml_logic.preprocessor import preprocess_features

from taxifare.ml_logic.model import (initialize_model,
                                           compile_model,
                                           train_model)

from taxifare.ml_logic.registry import (save_model,
                                              load_model)

import numpy as np
import pandas as pd
import os


def preprocess_and_train():
    """
    Load historical data in memory, clean and preprocess it, train a Keras model on it,
    save the model, and finally compute & save a performance metric
    on a validation set holdout at the `model.fit()` level
    """

    print("\n‚≠êÔ∏è use case: preprocess and train basic")


    # Retrieve raw data
    data_raw_path = os.path.join(LOCAL_DATA_PATH, "raw", f"train_{DATASET_SIZE}.csv")
    data = pd.read_csv(data_raw_path, dtype=DTYPES_RAW_OPTIMIZED)

    # Clean data using ml_logic.data.clean_data
    # $CODE_BEGIN
    data_cleaned = clean_data(data)
    # $CODE_END

    # Create X, y
    # $CODE_BEGIN
    X = data_cleaned.drop("fare_amount", axis=1)
    y = data_cleaned[["fare_amount"]]
    # $CODE_END

    # Preprocess X using `preprocessor.py`
    # $CODE_BEGIN
    X_processed = preprocess_features(X)
    # $CODE_END

    # Train model on X_processed and y, using `model.py`
    model = None
    learning_rate = 0.001
    batch_size = 256
    patience = 2
    # $CODE_BEGIN
    model = initialize_model(X_processed)
    model = compile_model(model, learning_rate)
    model, history = train_model(model, X_processed, y, batch_size=batch_size, patience=patience, validation_split=0.3)
    # $CODE_END

    # Compute the validation metric (min val mae of the holdout set)
    metrics = dict(mae=np.min(history.history['val_mae']))

    # Save trained model
    params = dict(
        learning_rate=learning_rate,
        batch_size=batch_size,
        patience=patience)
    save_model(model, params=params, metrics=metrics)

    # üß™ Write outputs so that they can be tested by make test_train_at_scale (do not remove)
    write_result(name="test_preprocess_and_train", subdir="train_at_scale", metrics=metrics)

    print("‚úÖ preprocess_and_train() done")

# $ERASE_BEGIN
def preprocess(source_type='train'):
    """
    Preprocess the dataset iteratively, loading data by chunks fitting in memory,
    processing each chunk, appending each of them to a final dataset preprocessed,
    and saving final prepocessed dataset as CSV
    Parameter:
    - source_type could be 'train' or 'val'
    """

    print("\n‚≠êÔ∏è use case: preprocess")

    # local saving paths given to you (do not overwrite these data_path variable)
    source_name = f"{source_type}_{DATASET_SIZE}.csv"
    destination_name = f"{source_type}_processed_{DATASET_SIZE}.csv"

    data_raw_path = os.path.abspath(os.path.join(LOCAL_DATA_PATH, "raw", source_name))
    data_processed_path = os.path.abspath(os.path.join(LOCAL_DATA_PATH, "processed", destination_name))

    # iterate on the dataset, by chunks
    chunk_id = 0

    # Let's loop until we reach the end of the dataset, then `break` out
    while (True):
        print(f"processing chunk n¬∞{chunk_id}...")


        try:
            # load in memory the chunk numbered `chunk_id` of size `CHUNK_SIZE`
            # üéØ Hint: check out pd.read_csv(skiprows=..., nrows=..., headers=...)
            # We advise you to always load data with `header=None`, and add back column names using COLUMN_NAMES_RAW

            # $CODE_BEGIN
            data_raw_chunk = pd.read_csv(
                    data_raw_path,
                    header=None,
                    skiprows=(chunk_id * CHUNK_SIZE) + 1, # first chunk has headers, we don't want them
                    nrows=CHUNK_SIZE,
                    dtype=DTYPES_RAW_OPTIMIZED_HEADLESS,
                    )

            assert dict(data_raw_chunk.dtypes) == DTYPES_RAW_OPTIMIZED_HEADLESS # read_csv(dtypes=...) will silently fail to convert data types, if column names do no match dictionnary key provided.

            data_raw_chunk.columns = COLUMN_NAMES_RAW
            # $CODE_END

        except pd.errors.EmptyDataError:
            # üéØ Hint: What would you do when you reached the end of the CSV ?
            # $CODE_BEGIN
            data_raw_chunk = None  # end of data
            # $CODE_END

        # $DEL_END
        # Break out of while loop if data is none
        if data_raw_chunk is None:
            break
        # $DEL_END

        # clean chunk
        # $CODE_BEGIN
        data_clean_chunk = clean_data(data_raw_chunk)
        # Break out of while loop if cleaning removed all rows
        if len(data_clean_chunk) == 0:
            break
        # $CODE_END

        # create X_chunk,y_chunk
        # $CODE_BEGIN
        X_chunk = data_clean_chunk.drop("fare_amount", axis=1)
        y_chunk = data_clean_chunk[["fare_amount"]]
        # $CODE_END

        # create X_processed_chunk and concatenate (X_processed_chunk, y_chunk) into data_processed_chunk
        # $CODE_BEGIN
        X_processed_chunk = preprocess_features(X_chunk)
        data_processed_chunk = pd.DataFrame(
            np.concatenate((X_processed_chunk, y_chunk), axis=1))
        # $CODE_END

        # Save and append the chunk of the preprocessed dataset to a local CSV
        # Keep headers on the first chunk
        # For convention, we'll always save CSVs with headers in this challenge
        # üéØ Hints: check out pd.to_csv(mode=...)
        # $CODE_BEGIN
        data_processed_chunk.to_csv(data_processed_path,
                mode="w" if chunk_id==0 else "a",
                header=chunk_id == 0, # Header only for first chunk
                index=False)
        # $CODE_END

        chunk_id += 1

    # üß™ Write outputs so that they can be tested by make test_train_at_scale (do not remove)
    data_processed = pd.read_csv(data_processed_path, header=None, skiprows=1, dtype=DTYPES_PROCESSED_OPTIMIZED).to_numpy()
    write_result(name="test_preprocess", subdir="train_at_scale", data_processed_head=data_processed[0:10])

    print("‚úÖ data processed saved entirely")


def train():
    """
    Training on the full (already preprocessed) dataset, by loading it
    chunk-by-chunk, and updating the weight of the model for each chunks.
    Save model, compute validation metrics on a holdout validation set that is
    common to all chunks.
    """
    print("\n ‚≠êÔ∏è use case: train")

    # Validation Set: Load a validation set common to all chunks and create X_val, y_val
    data_val_processed_path = os.path.abspath(os.path.join(
        LOCAL_DATA_PATH, "processed", f"val_processed_{VALIDATION_DATASET_SIZE}.csv"))

    data_val_processed = pd.read_csv(
        data_val_processed_path,
        skiprows= 1, # skip header
        header=None,
        dtype=DTYPES_PROCESSED_OPTIMIZED
        ).to_numpy()

    X_val = data_val_processed[:, :-1]
    y_val = data_val_processed[:, -1]

    # Iterate on the full training dataset chunk per chunks.
    # Break out of the loop if you receive no more data to train upon!
    model = None
    chunk_id = 0
    metrics_val_list = []  # store each metrics_val_chunk

    while (True):
        print(f"loading and training on preprocessed chunk n¬∞{chunk_id}...")

        # Load chunk of preprocess data and create (X_train_chunk, y_train_chunk)
        path = os.path.abspath(os.path.join(
            LOCAL_DATA_PATH, "processed", f"train_processed_{DATASET_SIZE}.csv"))

        try:
            data_processed_chunk = pd.read_csv(
                    path,
                    skiprows=(chunk_id * CHUNK_SIZE) + 1, # skip header
                    header=None,
                    nrows=CHUNK_SIZE,
                    dtype=DTYPES_PROCESSED_OPTIMIZED,
                    ).to_numpy()

        except pd.errors.EmptyDataError:
            data_processed_chunk = None  # end of data

        # Break out of while loop if we have no data to train upon
        if data_processed_chunk is None:
            break

        X_train_chunk = data_processed_chunk[:, :-1]
        y_train_chunk = data_processed_chunk[:, -1]

        learning_rate = 0.001
        batch_size = 256
        patience=2

        # Train a model *incrementally*, and store the val MAE of each chunk in `metrics_val_list`
        # $CODE_BEGIN
        if model is None:
            model = initialize_model(X_train_chunk)

        model = compile_model(model, learning_rate)
        model, history = train_model(model,
                                     X_train_chunk,
                                     y_train_chunk,
                                     batch_size=batch_size,
                                     patience=patience,
                                     validation_data=(X_val, y_val))
        metrics_val_chunk = np.min(history.history['val_mae'])
        metrics_val_list.append(metrics_val_chunk)
        print(metrics_val_chunk)
        # $CODE_END

        chunk_id += 1

    # return the last value of the validation MAE
    val_mae = metrics_val_list[-1]

    # Save model and training params
    params = dict(
        learning_rate=learning_rate,
        batch_size=batch_size,
        patience=patience,
        incremental=True,
        chunk_size=CHUNK_SIZE)

    print(f"\n‚úÖ trained with MAE: {round(val_mae, 2)}")

    save_model(model=model, params=params, metrics=dict(mae=val_mae))

    print("‚úÖ model trained and saved")

# $ERASE_END

def pred(X_pred: pd.DataFrame = None) -> np.ndarray:

    if X_pred is None:

        X_pred = pd.DataFrame(dict(
            key=["2013-07-06 17:18:00"],  # useless but the pipeline requires it
            pickup_datetime=["2013-07-06 17:18:00 UTC"],
            pickup_longitude=[-73.950655],
            pickup_latitude=[40.783282],
            dropoff_longitude=[-73.984365],
            dropoff_latitude=[40.769802],
            passenger_count=[1]))

    model = load_model()

    # preprocess the new data
    # $CODE_BEGIN
    X_processed = preprocess_features(X_pred)
    # $CODE_END

    # make a prediction
    # $CODE_BEGIN
    y_pred = model.predict(X_processed)
    # $CODE_END

    # üß™ Write outputs so that they can be tested by make test_train_at_scale (do not remove)
    write_result(name="test_pred", subdir="train_at_scale", y_pred=y_pred)
    print("‚úÖ prediction done: ", y_pred, y_pred.shape)

    return y_pred


if __name__ == '__main__':
    try:
        preprocess_and_train()
        pred()
    except:
        import ipdb, traceback, sys
        extype, value, tb = sys.exc_info()
        traceback.print_exc()
        ipdb.post_mortem(tb)
