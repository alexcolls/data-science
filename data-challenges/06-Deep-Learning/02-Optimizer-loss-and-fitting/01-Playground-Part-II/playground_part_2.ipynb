{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Goal**: Get a better understanding of ***Neural Network hyperparameters***\n",
    "\n",
    "<hr>\n",
    "\n",
    "üëâ Open the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.06711&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=false&regularizationRate_hide=false) again to learn more about Neural Networks. \n",
    "\n",
    "‚ùóÔ∏è Keep in mind that as the algorithm is stochastic, the results might differ from one run to the other. For this reason, do not hesitate to re-run the algorithms multiple times to analyse the behavior of your Neural Networks draw your conclusions accordingly.\n",
    "\n",
    "üïµüèª Let's explore the different items we have seen during the lecture:\n",
    "- **Batch Size**\n",
    "- **Regularization**\n",
    "- **Learning Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Initial Question** ‚ùì Select the `circle dataset` (Classification). \n",
    "\n",
    "* Build a model with: \n",
    "    * one hidden layer with 3 neurons,\n",
    "    * a _learning rate_ equal to 0.03, \n",
    "    * and the _tanh_ activation function\n",
    "\n",
    "* Do not add any noise (=0).\n",
    "\n",
    "* Select a batch size of 30\n",
    "\n",
    "***Look at the convergence of the algorithm. Does it seem slow or fast?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is happening with a batch size of 1** ‚ùì \n",
    "\n",
    "Now, run this neural network on the same dataset but... \n",
    "\n",
    "* with a batch-size of 1.\n",
    "* Make sure to run at least 150 epochs. \n",
    "\n",
    "***What do you notice on the train and test loss? What is the reason of this instability?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question/Observation** ‚ùì \n",
    "\n",
    "Now, you can see the effect of the _batch_size_ by reading the values of the train loss and test loss: pause the iterations and run it step by step (iteration per iteration) using to the `\"Step\"` button (at the right side of the play/stop button)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the lack of generalization** ‚ùì \n",
    "\n",
    "To observe once again the **lack of generalization**:\n",
    "* Select the `\"eXclusive OR\"(XOR)` dataset, \n",
    "* with a noise of 50. \n",
    "* Add a second hidden layer with again 8 neurons. \n",
    "\n",
    "***Try to fit your model once again... what do you expect ?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è With a smaller batch size, your model will end up overfitting faster... ‚ùóÔ∏è\n",
    "\n",
    "üëâ Let's keep ***`batch size = 1`*** though for the next question and try to understand how to prevent this overfitting phenomeon using the strategy of `regularization`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about regularization** ‚ùì \n",
    "\n",
    "Can we ***regularize*** our network to ***avoid overfitting***? \n",
    "\n",
    "* Keep the batch size to 1\n",
    "* Add a `L2-regularization`. \n",
    "* Increase the power of this L2-regularization until it smoothes out the predictions! \n",
    "Notice how the test loss doesn't increase anymore with the epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions with the spiral dataset** ‚ùì \n",
    "\n",
    "<u>Configuration</u>:\n",
    "\n",
    "* Select the `spiral dataset`,\n",
    "* Remove regularization, \n",
    "* Increase the `ratio of training to test data` to 80%. \n",
    "\n",
    "<u>Neural Network</u>: 3 hidden layers with:\n",
    "* 8 neurons on the first layer, \n",
    "* 7 neurons on the second layer,\n",
    "* 6 neurons on the third layer. \n",
    "\n",
    "<u>Experiment</u>:\n",
    "\n",
    "* Run the algorithm with a batch size of 30. \n",
    "* Make sure to run it for at least 1500 epochs. \n",
    "* Then, compare it to the same run but with a batch size of 1. \n",
    "\n",
    "You can check what happens on the train loss and test loss step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the <u>`circle dataset`</u>:\n",
    "* with no noise,\n",
    "* and a *ratio of training to test data* of 50%.\n",
    "* Use a batch size of 20. \n",
    "\n",
    "Use a <u>neural network</u> with:\n",
    "* one layer of 5 neurons,\n",
    "* no regularization, \n",
    "* and the tanh activation function\n",
    "\n",
    "‚ùì **Question about the learning rate** ‚ùì \n",
    "\n",
    "For each learning rate (from 0.0001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è When you change the learning rate, make sure to reinitialize the neural network (_circular arrow, left to the play/pause button_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "# test_loss = [UNCOMMENT THIS CELL AND STORE YOUR LOSS VALUES HERE]\n",
    "\n",
    "\n",
    "plt.plot(np.log(learning_rates), test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è Small and high learning rates both lead to a high test loss... but not for the same reasons !\n",
    "\n",
    "* Small Learning Rate helps a Neural Network converge in a similar fashion to a moderate learning rate but... way slower... i.e. more epochs would be needed!\n",
    "* Large Learning Rate makes the algorithm diverge completely.\n",
    "    - Try a learning rate $ \\alpha = 10 $ with 400 epochs, you should see of the values vary. This corresponds to the fact that the algorithms converge to *different local minima*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations !\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
