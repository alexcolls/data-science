{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing + Decision Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👩🏻‍🏫 In this challenge, we will combine:\n",
    "* 🗣 Natural Language Processing \n",
    "* 📊 Decision Science \n",
    "\n",
    "🎯 The goal is to understand the bad reviews of the products and the sellers on Olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_columns\",None)\n",
    "\n",
    "# Olist packages\n",
    "from olist.data import Olist\n",
    "from olist.review import Review\n",
    "from olist.order import Order\n",
    "from olist.product_updated import Product\n",
    "from olist.seller_updated import Seller\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Language Processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "# Vectorizers and NLP Models\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🕵🏻‍♂️ Imagine that the CEO of Olist [Tiago Dalvi](https://www.linkedin.com/in/tiagodalvi/) ask you to read and understand the reviews.\n",
    "\n",
    "- What did customers say about their order if they rated it 1? 2? 3?\n",
    "- What are the most frequent bad reviews about...\n",
    "    - the worst rated products ?\n",
    "    - the wort sellers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👩🏻‍🏫 If you followed the `Decision Science` module, you already have the `olist` package installed and importable. *You can skip this section* and move to the **Data collection** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.1) Import `olist` package`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a fresh version of the `olist` package:\n",
    "\n",
    "```bash\n",
    "mkdir ~/code/lewagon\n",
    "cd ~/code/lewagon\n",
    "git clone git@github.com:lewagon/olist.git\n",
    "cd olist\n",
    "git fetch\n",
    "git checkout full-package\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.2) Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the datasets from Kaggle https://www.kaggle.com/olistbr/brazilian-ecommerce\n",
    "- Unzip them into the `/data/csv` directory of the `olist` package:\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── README.md\n",
    "├── data\n",
    "│   └── csv\n",
    "│       ├── olist_customers_dataset.csv\n",
    "│       ├── olist_geolocation_dataset.csv\n",
    "│       ├── olist_order_items_dataset.csv\n",
    "│       ├── olist_order_payments_dataset.csv\n",
    "│       ├── olist_order_reviews_dataset.csv\n",
    "│       ├── olist_orders_dataset.csv\n",
    "│       ├── olist_products_dataset.csv\n",
    "│       ├── olist_sellers_dataset.csv\n",
    "│       └── product_category_name_translation.csv\n",
    "├── notebooks\n",
    "├── olist\n",
    "│   ├── README.md\n",
    "│   ├── __init__.py\n",
    "│   ├── data.py\n",
    "│   ├── order.py\n",
    "│   ├── product.py\n",
    "│   ├── product_updated.py\n",
    "│   ├── review.py\n",
    "│   ├── seller.py\n",
    "│   ├── seller_updated.py\n",
    "│   └── utils.py\n",
    "├── requirements.txt\n",
    "└── setup.p\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.3) Install the `olist` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "⚠️ Restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 The way we designed the `reviews` DataFrame will help us focus on the `product categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = Review().get_training_data()\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 We can retrieve the reviews from `order_reviews` within the Olist class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Olist().get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = data['order_reviews']\n",
    "reviews_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🪤 We need to collect some information about the orders because:\n",
    "- Back then, customers could review an order even before receiving it\n",
    "- They could also rate an order that they hadn't received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = data['orders']\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💥 Let's merge these three datasets and apply some operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚓 Remove the reviews that were written even before receiving the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚓 Remove the reviews for undelivered orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ As some people fill in only the title or the body of a review, it may be a good idea to aggregate them together.\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>💡<i>Hint</i></summary>\n",
    "\n",
    "* Drop rows with either a missing title or a missing comment\n",
    "* Refer to the documentation [pandas.DataFrame.dropna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) and have a look at the `how` argument..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🕵🏻 Let's focus on some columns of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on certain columns\n",
    "columns_of_interest = ['review_id',\n",
    "                       'length_review',\n",
    "                       'review_score',\n",
    "                       'order_id',\n",
    "                       'product_category_name',\n",
    "                       'full_review']\n",
    "df = df[columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧹 Create a function `cleaning(sentence)` and apply it to the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Analysis of bad reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1) Dataset with low review scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😱 What is the proportion of reviews with a rating between 1 and 3 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🕵🏻‍♂️ Let's focus on these reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔡 ➡️ 🔢 Vectorize your texts. \n",
    "\n",
    "* Make sure to take into accounts bigrams.\n",
    "* Set a `max_df` to $0.75$ to remove too frequent words\n",
    "* Spoiler alert: you will end up with $20k+$ words... let's keep only `max_features` = $5000$ for this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🕵🏻‍♂️ Fit an LDA:\n",
    "- Choose `n_components = 3`\n",
    "- Show the Document Mixture of Topics  with *.transform()*\n",
    "- Show the Topic Mixture with *.components_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Mixture (of Topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Let's report the most important topic for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Mixture (of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics\n",
    "\n",
    "🎁 We provided you with some functions:\n",
    "* `topic_word` returns the top words with their weights for one topic\n",
    "* `print_topics` prints the different topics found by the LDA with their topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word(vectorizer, model, topic, topwords, with_weights = True):\n",
    "    topwords_indexes = topic.argsort()[:-topwords - 1:-1]\n",
    "    if with_weights == True:\n",
    "        topwords = [(vectorizer.get_feature_names_out()[i], round(topic[i],2)) for i in topwords_indexes]\n",
    "    if with_weights == False:\n",
    "        topwords = [vectorizer.get_feature_names_out()[i] for i in topwords_indexes]\n",
    "    return topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(vectorizer, model, topwords):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"-\"*20)\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print(topic_word(vectorizer, model, topic, topwords))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🕵🏻‍♂️ Print the topics with their topwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🇧🇷 A bit of Brazilian Portuguese words here:\n",
    "- _cadeiras = chairs_\n",
    "- _producto = product_\n",
    "- _bom = good_\n",
    "- _comprei = bought_\n",
    "- _veio = came_\n",
    "- _duas = two_\n",
    "- _nao = not_\n",
    "- _entregue = delivered_\n",
    "- _pecas = part_\n",
    "- _ainda = yet_\n",
    "- _recebi = received_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Show the top words associated to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"most_important_words\"] = df[\"most_important_topic\"].apply(lambda i: topic_word_mixture[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"review_id\", \n",
    "        \"review_score\", \n",
    "        \"product_category_name\",\n",
    "        \"full_review_cleaned\",\n",
    "        \"most_important_topic\",\n",
    "        \"most_important_words\"]\n",
    "      ].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.4) Pipeline Tf-Idf and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔨 Create a Pipeline that chains your previous Vectorizer and the LDA.\n",
    "\n",
    "Fit it on the cleaned texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 If you try to access the components with `pipeline.components_`, it will NOT work because Pipeline doesn't have a `components_`. However, you can use `pipeline._final_estimator` to access the LDA. And from this, you can access the topics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline._final_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline._final_estimator.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document Mixture** with the Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Mixture** with the Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) 🎁 Product Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Groupby product categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📈 Group the dataset by `product_category_name` and inspect their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product categories by performance - look at the count, mean, median and std\n",
    "product_categories = df.groupby(by = 'product_category_name').agg({\n",
    "        'review_score': [\"count\", \"mean\", \"median\", \"std\"]\n",
    "    })\n",
    "\n",
    "# Removing products which were sold less than a certain times for the analysis\n",
    "cutoff = 50\n",
    "product_categories = product_categories[product_categories[(\"review_score\", \"count\")] > cutoff]\n",
    "\n",
    "# Sorting the product categories by performance\n",
    "product_categories = product_categories.sort_values(by = [('review_score', 'mean'), \n",
    "                                                          ('review_score', 'std')], \n",
    "                                                    ascending = [False, True])\n",
    "product_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) Worst product categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👎 Store the five worst categories in terms of *average review score* into a variable called `worst_products`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_products = product_categories.tail(5).sort_values(by = [(\"review_score\", \"count\")],\n",
    "                                                       ascending = False)\n",
    "worst_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Create a `worst_products_review` DataFrame which contains the `worst_products` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_products_reviews = df[df.product_category_name.isin(worst_products.index)]\n",
    "worst_products_reviews[[\"review_id\", \n",
    "                        \"review_score\", \n",
    "                        \"product_category_name\",\n",
    "                        \"full_review_cleaned\",\n",
    "                        \"most_important_topic\",\n",
    "                        \"most_important_words\"]\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.3). Topics for the worst products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What are the topics for the worst products ❓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_products_reviews[\"most_important_topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_frequency = list(worst_products_reviews[\"most_important_topic\"].value_counts().index)\n",
    "bad_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[topic_word_mixture[i] for i in bad_frequency]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 🎁 Sellers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What kind of products were sold by the worst sellers ?\n",
    "* What are the main reviews for the worst sellers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.1) Worst Sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = Seller().get_training_data()\n",
    "sellers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Select the 10 worst sellers and store them into a variable called `worst_sellers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_sellers = sellers[[\"seller_id\", \"review_score\", \"profits\"]].sort_values(\n",
    "    by = \"profits\", \n",
    "    ascending = True).head(10)\n",
    "worst_sellers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.2) Products sold by the worst sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = Product().get_training_data() [[\"product_id\", \"category\"]]\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What are the types of products sold by the worst sellers ❓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_product_category = data[\"order_items\"].merge(products, \n",
    "                                             on = \"product_id\", how = \"left\")[[\"seller_id\", \"category\"]]\n",
    "\n",
    "sellers_product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_product_category.groupby(\"seller_id\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.3) Categories and topics for the worst sellers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎁 Here are some useful functions:\n",
    "- `focus_seller(seller_id)` to show the product categories sold by a seller\n",
    "- `bad_reviews_seller` to show to topwords of the most frequent topic for one seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_seller(seller_id):\n",
    "    return sellers_product_category[sellers_product_category.seller_id == seller_id].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews_sellers = df.merge(data[\"order_items\"])\n",
    "bad_reviews_sellers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_reviews_seller(bad_reviews_sellers, seller_id):\n",
    "    mask = (bad_reviews_sellers.seller_id == seller_id)\n",
    "    temp = bad_reviews_sellers[mask]\n",
    "    most_frequent_topic_seller = list(temp.most_important_topic.value_counts().head(1).index)[0]\n",
    "    return topic_word_mixture[most_frequent_topic_seller]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓For each of these worst sellers, show the most frequent product categories and words ❓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worst_seller in worst_sellers[\"seller_id\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Focusing on the seller #{worst_seller}...\")\n",
    "    print(focus_seller(worst_seller))\n",
    "    print(bad_reviews_seller(bad_reviews_sellers, worst_seller))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 Congratulations. You've learned some basics of NLP (Preprocessing + Vectorizing + NB/LDA) and we combined this new \"expertise\" with Decision Science\n",
    "\n",
    "💾 Don't forget to `git add / commit / push`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
