{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code your own SVM with hand-made gradient descent üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by open-source notebook from Aurelien Geron [here](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ‚â•3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ‚â•0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64).reshape(-1, 1) # Iris virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "plt.scatter(X[:,0], X[:,1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Let's now try to code our own linear SVM ourselves to fit this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read about SVM cost function (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read \"Aurelien Geron - Hands-On Machine Learning with Scikit-Learn (2019)\" chapter 5 (Support Vector Machine)\n",
    "- Understand the cost function of the linear SVM, $J(\\mathbf{w}, b)$ below\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/lewagon/data-images/master/ML/svm_cost_function.png'>\n",
    "\n",
    "üëâ It requires you to buy the book, which is one of the best books in Data Science, well designed for Le Wagon students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Code your own Linear SVM model with hand-made Gradient Descent solver\n",
    "\n",
    "‚ùì Implement the model below, subclassing scikit-learn `BaseEstimator` class\n",
    "\n",
    "The `fit` method should minimize the cost function  $J(\\mathbf{w}, b)$ by gradient descent, after random initialization of its weights $\\mathbf{w}$ and bias $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyLinearSVC(BaseEstimator):\n",
    "    def __init__(self, C=1, eta0=1, eta_d=10000, n_epochs=1000):\n",
    "        self.C = C\n",
    "        self.eta0 = eta0\n",
    "        self.n_epochs = n_epochs\n",
    "        self.eta_d = eta_d\n",
    "\n",
    "    def eta(self, epoch):\n",
    "        return self.eta0 / (epoch + self.eta_d)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: Compute the weithgs w and b of the linear SVM by minimizing the cost function\n",
    "        pass\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # TODO: What scalar should you compute so as to predict the class correctly ?\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) >= 0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your gradient descent\n",
    "model_custom = MyLinearSVC(C=2, eta0 = 10, eta_d = 1000, n_epochs=60000)\n",
    "model_custom.fit(X, y)\n",
    "model_custom.predict(np.array([[5, 2], [4, 1]]))\n",
    "\n",
    "plt.plot(range(model_custom.n_epochs), model_custom.Js)\n",
    "plt.axis([0, model_custom.n_epochs, 0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) üß™ Compare with scikit-learn default SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scikit = SVC(kernel=\"linear\", C=2)\n",
    "model_scikit.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that both models predict the same classes\n",
    "assert (np.sum(np.abs(model_custom.predict(X).ravel() - model_scikit.predict(X).ravel())) == 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that both models coefficients are quite close to each other\n",
    "print(model_custom.intercept_, model_custom.coef_)\n",
    "print(model_scikit.intercept_, model_scikit.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Note that `scikit-learn` Linear SVC is much faster to converge, as it is optimized via quadratic solving instead of gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulation!** Don't forget to push and commit your results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
