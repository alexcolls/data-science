{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Exercise objectives**\n",
    "- Discover ***autoencoders***\n",
    "- Get a deeper understanding of CNNs\n",
    "\n",
    "<hr>\n",
    "\n",
    "üëâ There exists a very particular architecture in Deep Learning called **`Autoencoders`**. Autoencoders are Neural Network architectures trained to return **outputs that are as similar as possible to the original inputs fed to them**. Why would we do that?  \n",
    "\n",
    "Before answering the question _\"why\"_, let's answer the question _\"how\"_.\n",
    "\n",
    "üë©üèª‚Äçüè´ <u>***How does an autoencoder work ?***</u>\n",
    "\n",
    "There are two parts in an autoencoder: the  **`encoder`** and the **`decoder`**.\n",
    "\n",
    "1. In the encoder, we will make the information flow through different dense layers with a decreasing number of neurons. It will create a **`bottleneck`** where the information is compressed.\n",
    "\n",
    "2. In the decoder, we will try to recreate the original data based on the compressed data.\n",
    "\n",
    "üî• <u>***Why is it powerful or useful?***</u>\n",
    "\n",
    "If it works well, it means two important things:\n",
    "\n",
    "* ‚úÖ We can afford to **compress our dataset** and use a compressed version of it when fitting another Neural Network! \n",
    "\n",
    "* ‚úÖ The **information contained in the bottleneck** - i.e. the data compressed in a low-dimensional layer - **accurately captures the patterns of our dataset** and the autoencoder is able to decode the compressed information!\n",
    "\n",
    "üå† <u>**Applications:**</u>\n",
    "- Image compression\n",
    "- Denoising (cf. Google Pixel phones...)\n",
    "- Image generation!\n",
    "\n",
    "\n",
    "<img src='https://github.com/lewagon/data-images/blob/master/DL/autoencoder.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The MNIST Dataset\n",
    "\n",
    "In this notebook, we will train an auto-encoder to work on 28x28 grey images from the MNIST dataset, available in Keras. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "print(images_train.shape)\n",
    "print(images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a channels for the colors and normalize data\n",
    "X_train = images_train.reshape((60000, 28, 28, 1)) / 255.\n",
    "X_test = images_test.reshape((10000, 28, 28, 1)) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axs = plt.subplots(1, 10, figsize=(20, 4))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='Greys')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ First, we built the \"Encoder\" part for you.\n",
    "\n",
    "üëâ  Notice how similar it looks compared to a Convolutional Classifier with **latent_dimension** neurons at the end. However, we using the \"tanh\" activation function in the final dense layer instead of \"relu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def build_encoder(latent_dimension):\n",
    "    '''returns an encoder model, of output_shape equals to latent_dimension'''\n",
    "    encoder = Sequential()\n",
    "    \n",
    "    encoder.add(Conv2D(8, (2,2), input_shape=(28, 28, 1), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "\n",
    "    encoder.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "\n",
    "    encoder.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))     \n",
    "\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(latent_dimension, activation='tanh'))\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: building an encoder** ‚ùì \n",
    "\n",
    "Build your encoder with **`latent_dimension = 2`** and look at the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn to build the decoder this time!\n",
    "\n",
    "We need to build a üî• **`reversed CNN` üî•** that \n",
    "* takes a dense layer as input,\n",
    "* and outputs an image of shape $ (28,28,1) $ similar to our MNIST images. \n",
    "\n",
    "üìö For this purpose, we will use a new layer called <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\">**`Conv2DTranspose`**</a> üìö\n",
    "    \n",
    "The name of this layer speaks for itself: it performs the opposite of a convolution operation!\n",
    "\n",
    "üí° We will follow this strategy:\n",
    "* Start by reshaping the Dense Input Layer into an Image of shape $(7,7,..)$\n",
    "* Then apply the `Conv2DTranspose` operation with ***strides = 2*** to double the output shape to $(14,14,..)$\n",
    "* then add another Conv2DTranpose layer on top of the first one to make it $(28,28,1)$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "‚ùì **Question: the architecture of a decoder** ‚ùì \n",
    "\n",
    "\n",
    "Define a **`decoding architecture`** in the method below as follows:\n",
    "- a *Dense* layer with:\n",
    "    - $7 \\times 7 \\times 8$ neurons, \n",
    "    - *input_shape* = (latent_dimension, )\n",
    "    - *tanh* activation function. \n",
    "- a *Reshape* layer that reshapes to $(7, 7, 8)$ tensors\n",
    "- a *Conv2DTranspose* with:\n",
    "    - $8$ filters, \n",
    "    - $(2,2)$ kernels, \n",
    "    - strides of $2$, \n",
    "    - padding *same* \n",
    "    - _relu_ activation function\n",
    "- a second Conv2DTranspose layer with:\n",
    "    - $1$ filter,\n",
    "    - $(2,2)$ kernels,\n",
    "    - strides of $2$,\n",
    "    - padding _same_,\n",
    "    - _relu_ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "\n",
    "def build_decoder(latent_dimension):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: buiding a decoder** ‚ùì \n",
    "\n",
    "Build your decoder with **`latent_dimension = 2`** and check that it outputs images of same shape than the encoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ We can now **concatenate** both **`the encoder and the decoder`** thanks to the **`Model`** class in Keras, using the **`Functional API`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def build_autoencoder(encoder, decoder):\n",
    "    inp = Input((28, 28,1))\n",
    "    encoded = encoder(inp)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(inp, decoded)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions** ‚ùì \n",
    "\n",
    "* Try to understand syntax above üëÜ \n",
    "* Build your autoencoder\n",
    "* Have a look at the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Compiling an autoencoder** ‚ùì \n",
    "\n",
    "Define a method which compiles your model. Pick an appropriate loss.\n",
    "\n",
    "<u><i>Think carefully:</i></u> ü§î On which mathematical object are we going to compare *predictions* and the *ground truth* for the computation of the loss function and the metrics?\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "It should compare two images (Black and White in our case), pixel-by-pixel!\n",
    "    \n",
    "The MSE loss seems to be an appropriate loss function for pixel-by-pixel error minimization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Training an autoencoder** ‚ùì  \n",
    "\n",
    "* Compile your model and fit it with `batch_size = 32` and `epochs = 20`. \n",
    "* What is the label `y_train` in this case?\n",
    "\n",
    "<i>Note:</i> Don't waste your time fighting overfitting in this challenge, you will have time to care about this during the project weeks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Encoding the dataset** ‚ùì\n",
    "\n",
    "* Using only the encoder part of the network, encode your dataset and save it under `X_encoded` . \n",
    "    * Each image is now represented by two values (that correspond to the dimension of the latent space, of the bottleneck; aka the `latent_dimension`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Where are we after running the encoder?\n",
    "\n",
    "* Each image was compressed into a 2D space. \n",
    "* Each of these handwritten digit have a given label, between 0 and 9, but the goal here is not to classify these pictures like in the first challenge but to **reconstruct the original image before the compression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Visualizing handwritten digits in the latent space** ‚ùì \n",
    "\n",
    "Scatterplot the encoded data (only a small fraction of the encoded dataset for visibility purposes...)\n",
    "- Each point of the scatter plot  corresponds to an encoded image\n",
    "- Color the dots according to their respective labels (digit representation):\n",
    "    - for instance, all the \"4\"s should be represented by a color on this scatter plot...\n",
    "    - ...while the \"5\" should be represented by another color\n",
    "    - choose a set of [`qualitative colormaps`](https://matplotlib.org/stable/gallery/color/colormap_reference.html)\n",
    "\n",
    "What do you remark about this plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Application: Image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions: Creating some noise in the dataset** ‚ùì \n",
    "\n",
    "* Let's add some noise to the input data. \n",
    "* Run the following code\n",
    "* Plot some handwritten digits and their noisy versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "noise_factor = 0.5\n",
    "\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(0., 1., size=X_train.shape)\n",
    "X_test_noisy = X_test + noise_factor * np.random.normal(0., 1., size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: decoding the noisy pictures** ‚ùì \n",
    "\n",
    "* Reinitialize your autoencoder (with a latent space of 2) \n",
    "* Train it again, this time using the noisy train dataset instead of the normal train dataset\n",
    "    * *Keep `batch_size = 32` and `epochs = 5`*\n",
    "* What do you expect if you run the autoencoder on the noisy data instead of the original data in terms of performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: comparing the noisy test images with the denoised images** ‚ùì \n",
    "\n",
    "For some noisy test images, predict the denoised images and plot the results side by side..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: choosing the \"correct\" latent_dimension** ‚ùì \n",
    "\n",
    "Now, try to evaluate which **`latent_dimension`** is the most suitable in order to have **`the best image reconstruction preprocess`** $ \\Leftrightarrow $ How to remove as much noise as possible in the noisy dataset using the latent dimension?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü•° <b><u>Conclusion</u></b>\n",
    "\n",
    "\n",
    "* It is obvious that:\n",
    "    * if you compress your pictures of size $ 28 \\times 28 $ into a 1D space, you will lose a ton of information. \n",
    "    * if you compress them into a $ 28 \\times 28 = 784$ -space, you are actually not compressing them\n",
    "    \n",
    "* We can still use this graph of **Loss vs. Latent dimensions** reading it from right to left to decide in which latent space it would be advisable to compress the pictures without losing to much information: `latent_space = 8` seems a sweat spot here using the Elbow Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üèÅ **Congratulations** üèÅ \n",
    "\n",
    "1. Download this notebook from your `Google Drive` or directly from `Google Colab` \n",
    "2. Drag-and-drop it from your `Downloads` folder to your local `[GITHUB_USERNAME]/data-challenges/06-Deep-Learning/03-Convolutional-Neural-Networks/05-autoencoders`\n",
    "\n",
    "\n",
    "üíæ Don't forget to push your code\n",
    "\n",
    "3. Follow the usual procedure on your terminal in the `06-Deep-Learning/03-Convolutional-Neural-Networks/05-autoencoders` folder:\n",
    "      * *git add cifar_classification.ipynb*\n",
    "      * *git commit -m \"I am the god of Transfer Learning\"*\n",
    "      * *git push origin master*\n",
    "\n",
    "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
    "\n",
    "üòâ That was the last challenge of this module!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
